WEBVTT
Kind: captions
Language: en

00:00:01.340 --> 00:00:04.485
Now, your monitoring server is equipped with

00:00:04.485 --> 00:00:07.830
Alertmanager and you can send out alerts when needed.

00:00:07.830 --> 00:00:09.630
If you'd like to experiment some more,

00:00:09.630 --> 00:00:13.260
try adding another communication channel to diversify your alerts.

00:00:13.260 --> 00:00:18.015
You could also experiment with the PromQL expressions to trigger more alerts.

00:00:18.015 --> 00:00:20.580
Remember, what I said about alert fatigue?

00:00:20.580 --> 00:00:24.960
It's a real thing and can drain your monitoring system of its value,

00:00:24.960 --> 00:00:27.750
once your team starts to ignore its alerts.

00:00:27.750 --> 00:00:31.980
Keep your alerts relevant, actionable, and important.

00:00:31.980 --> 00:00:36.480
Anything else can just go to a dashboard for people to view when they want to.

00:00:36.480 --> 00:00:38.570
If you're struggling with this exercise,

00:00:38.570 --> 00:00:41.975
go back to the alert section and re-watch those videos.

00:00:41.975 --> 00:00:46.870
Let's actually take a look at this system working and alerting.

00:00:46.870 --> 00:00:48.510
As I've shown you before,

00:00:48.510 --> 00:00:51.710
when Alertmanager is not alerting on anything,

00:00:51.710 --> 00:00:53.090
there are no alerts going on,

00:00:53.090 --> 00:00:56.375
no problems, then Alertmanager is actually pretty boring.

00:00:56.375 --> 00:00:58.160
There's nothing really going on here.

00:00:58.160 --> 00:01:01.270
Let's actually cause something to happen.

00:01:01.270 --> 00:01:03.510
Here in my AWS console,

00:01:03.510 --> 00:01:05.600
I've got four instances running.

00:01:05.600 --> 00:01:09.560
I've got Prometheus, and I've got three node exporters running.

00:01:09.560 --> 00:01:12.380
Let's actually take one of these out of commission.

00:01:12.380 --> 00:01:15.625
We'll just go to Actions and stop the instance.

00:01:15.625 --> 00:01:17.835
Yeah, let's stop that.

00:01:17.835 --> 00:01:22.325
Now, this is going to do something to Prometheus.

00:01:22.325 --> 00:01:24.955
Let's take a look at Prometheus to see what's happening.

00:01:24.955 --> 00:01:29.180
Here in Prometheus, we should be able to go to alerts and

00:01:29.180 --> 00:01:33.805
see that we have one active alert InstanceDown.

00:01:33.805 --> 00:01:37.510
Here, we can see the expression is running up equals 0,

00:01:37.510 --> 00:01:40.565
which means the instance is not working anymore,

00:01:40.565 --> 00:01:43.400
and we can see that it's pending state.

00:01:43.400 --> 00:01:45.725
Since this time in history,

00:01:45.725 --> 00:01:48.915
this instance has been down.

00:01:48.915 --> 00:01:51.785
Now, it's only pending, because remember,

00:01:51.785 --> 00:01:55.925
we want this status for one whole minute.

00:01:55.925 --> 00:01:58.790
We need to wait one minute and then we'll be able to see

00:01:58.790 --> 00:02:01.645
that this alert goes over to Alertmanager.

00:02:01.645 --> 00:02:04.670
Here in Alertmanager, we're able to see that we have

00:02:04.670 --> 00:02:07.730
a pending alert almost ready to send out.

00:02:07.730 --> 00:02:10.565
We're just waiting for that minute to expire.

00:02:10.565 --> 00:02:12.980
I just heard a notification from Slack.

00:02:12.980 --> 00:02:15.060
Let's go check it out. There it is.

00:02:15.060 --> 00:02:18.685
Right here in Slack, we've got an alert telling us that we have an InstanceDown.

00:02:18.685 --> 00:02:20.875
Now, let's go fix it.

00:02:20.875 --> 00:02:26.590
To fix this, all we got to do is start our node exporter again.

00:02:30.940 --> 00:02:34.400
Once the EC2 instance is back up and running,

00:02:34.400 --> 00:02:37.800
we no longer have any alerts in Alertmanager,

00:02:37.800 --> 00:02:40.115
and we see here in Prometheus,

00:02:40.115 --> 00:02:44.570
that our alert for InstanceDown is no longer pending or active.

00:02:44.570 --> 00:02:47.780
Now, my dev team and I are poised and ready to be able to

00:02:47.780 --> 00:02:52.560
solve problems on the fly without having long periods of downtime.

